<div align="center">

# ğŸ¬ IMDb Sentiment Analysis with DistilBERT

### ğŸš€ Fine-Tuning Large Language Models for Movie Review Classification

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.30+-yellow.svg)](https://huggingface.co/transformers/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

*Achieving 93%+ accuracy on binary sentiment classification through strategic hyperparameter optimization*

[ğŸ¯ Overview](#-overview) â€¢ [ğŸ“Š Results](#-results) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“ Project Structure](#-project-structure) â€¢ [ğŸ“ Methodology](#-methodology)

</div>

---

## ğŸ¯ Overview

This project demonstrates **state-of-the-art fine-tuning techniques** for sentiment analysis using DistilBERT on the IMDb movie review dataset. Through systematic hyperparameter optimization and rigorous evaluation, we achieved a **43+ percentage point improvement** over the baseline model.

### âœ¨ Key Highlights

- ğŸ¯ **93.04% Accuracy** - Best performing configuration
- âš¡ **66M Parameters** - Efficient DistilBERT architecture
- ğŸ”¬ **3 Configurations** - Comprehensive hyperparameter search
- ğŸ“ˆ **+43pp Improvement** - Significant baseline enhancement
- ğŸ¨ **Production Ready** - Complete inference pipeline included

---

## ğŸ“Š Results

### ğŸ† Performance Comparison

| Configuration | Accuracy | F1 Score | Precision | Recall | Training Time |
|--------------|----------|----------|-----------|---------|---------------|
| ğŸ”µ Baseline (No Fine-tuning) | 50.00% | 0.5000 | 0.5000 | 0.5000 | - |
| ğŸŸ¢ **Config 1: Standard** | **93.04%** | **0.9304** | **0.9305** | **0.9304** | 15.2 min |
| ğŸŸ¡ Config 2: High LR | 92.88% | 0.9288 | 0.9289 | 0.9288 | 15.0 min |
| ğŸŸ£ Config 3: Small Batch | 92.96% | 0.9296 | 0.9297 | 0.9296 | 20.3 min |

### ğŸ“ˆ Key Insights

- âœ… **Standard fine-tuning** (Config 1) achieved the best results
- âœ… **Higher learning rates** showed minimal performance trade-off
- âœ… **Smaller batches** with more regularization maintained competitive accuracy
- âœ… **All configurations** significantly outperformed the baseline

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites
```bash
# Python 3.8+
pip install transformers datasets torch accelerate evaluate scikit-learn pandas matplotlib seaborn
```

### âš¡ Run in Google Colab

1. **Open Colab** and enable GPU (`Runtime` â†’ `Change runtime type` â†’ `T4 GPU`)
2. **Clone & Setup**:
```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install dependencies
!pip install -q transformers datasets accelerate evaluate scikit-learn
```

3. **Load & Run** the complete notebook (see [notebooks/](notebooks/) folder)

### ğŸ¯ Using the Fine-Tuned Model
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load model
model = AutoModelForSequenceClassification.from_pretrained('./models/config_1/final_model')
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

# Make prediction
text = "This movie was absolutely fantastic! I loved every minute of it."
inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

with torch.no_grad():
    outputs = model(**inputs)
    prediction = torch.argmax(outputs.logits, dim=-1).item()
    
print(f"Sentiment: {'Positive ğŸ˜Š' if prediction == 1 else 'Negative ğŸ˜'}")
```

**Output:**
```
Sentiment: Positive ğŸ˜Š
```

---

## ğŸ“ Project Structure
```
imdb_sentiment_finetuning/
â”‚
â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â””â”€â”€ fine_tuning_complete.ipynb    # Complete training notebook
â”‚
â”œâ”€â”€ ğŸ¤– models/
â”‚   â”œâ”€â”€ config_1/final_model/         # Best model (93.04% accuracy)
â”‚   â”œâ”€â”€ config_2/final_model/         # High LR variant
â”‚   â””â”€â”€ config_3/final_model/         # Small batch variant
â”‚
â”œâ”€â”€ ğŸ“Š results/
â”‚   â”œâ”€â”€ baseline_results.json         # Pre-fine-tuning metrics
â”‚   â”œâ”€â”€ config_1_results.json         # Config 1 performance
â”‚   â”œâ”€â”€ config_2_results.json         # Config 2 performance
â”‚   â”œâ”€â”€ config_3_results.json         # Config 3 performance
â”‚   â””â”€â”€ comparison.csv                # Side-by-side comparison
â”‚
â”œâ”€â”€ ğŸ“ˆ visualizations/
â”‚   â”œâ”€â”€ dataset_statistics.png        # Data distribution plots
â”‚   â”œâ”€â”€ performance_comparison.png    # Metrics bar charts
â”‚   â”œâ”€â”€ confusion_matrix.png          # Classification results
â”‚   â””â”€â”€ error_patterns.png            # Error analysis
â”‚
â”œâ”€â”€ ğŸ” error_analysis/
â”‚   â””â”€â”€ detailed_errors.csv           # Misclassified examples
â”‚
â”œâ”€â”€ ğŸ“ logs/
â”‚   â”œâ”€â”€ config_1/                     # TensorBoard logs
â”‚   â”œâ”€â”€ config_2/
â”‚   â””â”€â”€ config_3/
â”‚
â”œâ”€â”€ ğŸ inference_script.py            # Production inference code
â”œâ”€â”€ ğŸ“„ FINAL_SUMMARY_REPORT.txt       # Comprehensive report
â””â”€â”€ ğŸ“– README.md                       # This file
```

---

## ğŸ“ Methodology

### 1ï¸âƒ£ Dataset Preparation

- **Source**: IMDb Movie Reviews Dataset
- **Size**: 50,000 reviews (25k train, 5k validation, 25k test)
- **Split**: 80% training, 20% validation from original training set
- **Preprocessing**: Tokenization with max length 512, padding and truncation
- **Balance**: 50/50 positive/negative sentiment distribution

### 2ï¸âƒ£ Model Architecture
```
ğŸ—ï¸ DistilBERT (distilbert-base-uncased)
â”œâ”€â”€ ğŸ“¦ Parameters: 66,955,010
â”œâ”€â”€ ğŸ”¢ Layers: 6 transformer layers
â”œâ”€â”€ ğŸ¯ Output: Binary classification (positive/negative)
â””â”€â”€ âš¡ Efficiency: 40% smaller, 60% faster than BERT
```

### 3ï¸âƒ£ Hyperparameter Configurations

#### ğŸŸ¢ Configuration 1: Standard Fine-Tuning
```python
Learning Rate: 2e-5
Batch Size: 16
Epochs: 3
Weight Decay: 0.01
Warmup Steps: 500
```

#### ğŸŸ¡ Configuration 2: Higher Learning Rate
```python
Learning Rate: 5e-5  # 2.5x higher
Batch Size: 16
Epochs: 3
Weight Decay: 0.01
Warmup Steps: 500
```

#### ğŸŸ£ Configuration 3: Smaller Batch + More Regularization
```python
Learning Rate: 3e-5
Batch Size: 8       # 50% smaller
Epochs: 4           # +1 epoch
Weight Decay: 0.02  # 2x higher
Warmup Steps: 500
```

### 4ï¸âƒ£ Training Strategy

- âœ… **Mixed Precision Training** (FP16) for faster computation
- âœ… **Early Stopping** with patience of 3 evaluations
- âœ… **Best Model Selection** based on F1 score
- âœ… **Gradient Checkpointing** for memory efficiency
- âœ… **Learning Rate Warmup** for stable training

### 5ï¸âƒ£ Evaluation Metrics

- ğŸ“Š **Accuracy**: Overall correctness
- ğŸ¯ **F1 Score**: Harmonic mean of precision and recall
- ğŸ” **Precision**: Accuracy of positive predictions
- ğŸ“ˆ **Recall**: Coverage of actual positives
- ğŸ§© **Confusion Matrix**: Detailed error breakdown

---

## ğŸ”¬ Error Analysis

### ğŸ“‰ Error Distribution

- **Total Errors**: 1,740 out of 25,000 (6.96%)
- **False Positives**: 870 (50%)
- **False Negatives**: 870 (50%)

### ğŸ¯ Common Error Patterns

1. **Sarcastic Reviews** ğŸ˜
   - Model struggles with subtle sarcasm and irony
   - Example: "Oh great, another predictable plot twist..."

2. **Mixed Sentiment** ğŸ¤”
   - Reviews with both positive and negative aspects
   - Example: "Great acting, but terrible storyline"

3. **Short Reviews** ğŸ“
   - Limited context makes classification challenging
   - Example: "Okay." or "Not bad."

4. **Nuanced Language** ğŸ­
   - Complex vocabulary and subtle expressions
   - Example: "A bittersweet meditation on..."

### ğŸ’¡ Suggested Improvements

- ğŸ”„ **Data Augmentation**: Add paraphrased versions of misclassified examples
- ğŸ¤ **Ensemble Methods**: Combine predictions from multiple configurations
- ğŸ¯ **Domain Adaptation**: Pre-train on additional movie review corpora
- ğŸ”§ **Advanced Techniques**: Implement LoRA for parameter-efficient fine-tuning

---

## ğŸ“¸ Visualizations

<div align="center">

### ğŸ“Š Performance Metrics

![Performance Comparison](visualizations/performance_comparison.png)

### ğŸ¯ Confusion Matrix

![Confusion Matrix](visualizations/confusion_matrix.png)

### ğŸ“ˆ Dataset Statistics

![Dataset Statistics](visualizations/dataset_statistics.png)

</div>

---

## ğŸ› ï¸ Technical Stack

| Component | Technology |
|-----------|-----------|
| ğŸ§  **Deep Learning** | PyTorch, Transformers (Hugging Face) |
| ğŸ“Š **Data Processing** | Pandas, NumPy, Datasets |
| ğŸ“ˆ **Visualization** | Matplotlib, Seaborn |
| ğŸ¯ **Evaluation** | scikit-learn |
| â˜ï¸ **Infrastructure** | Google Colab (T4 GPU) |
| ğŸ’¾ **Storage** | Google Drive |

---

## ğŸ¥ Demo

### ğŸ–¥ï¸ Command Line Interface
```python
# Run inference on custom text
python inference_script.py --text "This movie exceeded all my expectations!"

# Output:
# ğŸ¬ Sentiment Analysis Result
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Input: "This movie exceeded all my expectations!"
# Prediction: Positive ğŸ˜Š
# Confidence: 98.7%
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ“š Documentation

### ğŸ“– Key Files

- ğŸ“„ **FINAL_SUMMARY_REPORT.txt** - Comprehensive technical report
- ğŸ““ **notebooks/fine_tuning_complete.ipynb** - Complete training pipeline
- ğŸ **inference_script.py** - Production-ready inference code
- ğŸ“Š **results/comparison.csv** - Detailed metrics comparison

### ğŸ“ Academic Context

This project was completed as part of a **Large Language Model Fine-Tuning** assignment, demonstrating:

- âœ… Systematic hyperparameter optimization
- âœ… Rigorous evaluation methodology
- âœ… Professional documentation practices
- âœ… Production-ready implementation

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. ğŸ´ **Fork** the repository
2. ğŸŒ¿ **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ **Commit** your changes (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ **Push** to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ‰ **Open** a Pull Request

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- ğŸ¤— **Hugging Face** for the Transformers library
- ğŸ¬ **IMDb** for the movie review dataset
- ğŸ§  **DistilBERT** authors for the efficient architecture
- â˜ï¸ **Google Colab** for free GPU access

---

## ğŸ“ Contact

**Mayuresh Satao** - satao.m@northeastern.edu(mailto:satao.m@northeastern.edu)

Project Link: [https://github.com/mayureshsatao/Distilbert-IMDB-Sentiment-Analysis](https://github.com/mayureshsatao/Distilbert-IMDB-Sentiment-Analysis)

---

<div align="center">

### ğŸŒŸ If you found this project helpful, please give it a star! ğŸŒŸ

**Made with â¤ï¸ and ğŸ¤–**

[![GitHub Stars](https://img.shields.io/github/stars/mayureshsatao/Distilbert-IMDB-Sentiment-Analysis?style=social)](https://github.com/mayureshsatao/Distilbert-IMDB-Sentiment-Analysis)
[![GitHub Forks](https://img.shields.io/github/forks/mayureshsatao/Distilbert-IMDB-Sentiment-Analysis?style=social)](https://github.com/mayureshsatao/Distilbert-IMDB-Sentiment-Analysis)

</div>